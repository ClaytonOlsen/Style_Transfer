{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess_util as util\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from train_premade_style import *\n",
    "from preprocess_util import *\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "from os.path import exists\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Training loss (for one batch) at step 1: 25872623145582592.0000\n",
      "Training loss (for one batch) at step 2: 339571315696467968.0000\n",
      "Training loss (for one batch) at step 3: 140847044381114368.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 63495645452828672.0000\n",
      "Training loss (for one batch) at step 2: 32023057115643904.0000\n",
      "Training loss (for one batch) at step 3: 22376085974941696.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 33166688794968064.0000\n",
      "Training loss (for one batch) at step 2: 21774187110596608.0000\n",
      "Training loss (for one batch) at step 3: 19816092192997376.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 19955824659005440.0000\n",
      "Training loss (for one batch) at step 2: 20057456470130688.0000\n",
      "Training loss (for one batch) at step 3: 14698470082347008.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 15139607247060992.0000\n",
      "Training loss (for one batch) at step 2: 14668818701877248.0000\n",
      "Training loss (for one batch) at step 3: 13653243619966976.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 13042478869381120.0000\n",
      "Training loss (for one batch) at step 2: 12841199522021376.0000\n",
      "Training loss (for one batch) at step 3: 12449503873335296.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 11924126293819392.0000\n",
      "Training loss (for one batch) at step 2: 11112273321918464.0000\n",
      "Training loss (for one batch) at step 3: 10703260365094912.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 10380422705840128.0000\n",
      "Training loss (for one batch) at step 2: 9545359730671616.0000\n",
      "Training loss (for one batch) at step 3: 8664892508733440.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 8273818556563456.0000\n",
      "Training loss (for one batch) at step 2: 7911323316781056.0000\n",
      "Training loss (for one batch) at step 3: 7862472895627264.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 7840374215147520.0000\n",
      "Training loss (for one batch) at step 2: 8202252053381120.0000\n",
      "Training loss (for one batch) at step 3: 7593880136450048.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 9104281621757952.0000\n",
      "Training loss (for one batch) at step 2: 8705874549800960.0000\n",
      "Training loss (for one batch) at step 3: 8061318070272000.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 8534083374153728.0000\n",
      "Training loss (for one batch) at step 2: 8549742288044032.0000\n",
      "Training loss (for one batch) at step 3: 7774921061040128.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 7388304748052480.0000\n",
      "Training loss (for one batch) at step 2: 7023380704264192.0000\n",
      "Training loss (for one batch) at step 3: 6511533882343424.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 6651240041676800.0000\n",
      "Training loss (for one batch) at step 2: 6534917831786496.0000\n",
      "Training loss (for one batch) at step 3: 6150482825314304.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 6094245060411392.0000\n",
      "Training loss (for one batch) at step 2: 6060502186721280.0000\n",
      "Training loss (for one batch) at step 3: 5890300148973568.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 5834270689984512.0000\n",
      "Training loss (for one batch) at step 2: 5579425584250880.0000\n",
      "Training loss (for one batch) at step 3: 5350834539855872.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 5361387811373056.0000\n",
      "Training loss (for one batch) at step 2: 5132257748582400.0000\n",
      "Training loss (for one batch) at step 3: 4979692457164800.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 4887055381299200.0000\n",
      "Training loss (for one batch) at step 2: 4728570551205888.0000\n",
      "Training loss (for one batch) at step 3: 4568620600393728.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 4452098406088704.0000\n",
      "Training loss (for one batch) at step 2: 4346119014318080.0000\n",
      "Training loss (for one batch) at step 3: 4150994623528960.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 4004414369038336.0000\n",
      "Training loss (for one batch) at step 2: 3753050871169024.0000\n",
      "Training loss (for one batch) at step 3: 3414790521225216.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 3043971634823168.0000\n",
      "Training loss (for one batch) at step 2: 2538595718004736.0000\n",
      "Training loss (for one batch) at step 3: 1941014369533952.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 1112639732187136.0000\n",
      "Training loss (for one batch) at step 2: 349177317949440.0000\n",
      "Training loss (for one batch) at step 3: 178748737454080.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 148161607761920.0000\n",
      "Training loss (for one batch) at step 2: 62919454752768.0000\n",
      "Training loss (for one batch) at step 3: 33595523596288.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 32293318033408.0000\n",
      "Training loss (for one batch) at step 2: 31104822149120.0000\n",
      "Training loss (for one batch) at step 3: 31514800685056.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 30869754478592.0000\n",
      "Training loss (for one batch) at step 2: 30085105057792.0000\n",
      "Training loss (for one batch) at step 3: 30757554749440.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 30331879030784.0000\n",
      "Training loss (for one batch) at step 2: 29669933973504.0000\n",
      "Training loss (for one batch) at step 3: 30300388196352.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29661629251584.0000\n",
      "Training loss (for one batch) at step 2: 28870164086784.0000\n",
      "Training loss (for one batch) at step 3: 29627479228416.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29304268259328.0000\n",
      "Training loss (for one batch) at step 2: 28776433975296.0000\n",
      "Training loss (for one batch) at step 3: 29606293798912.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297538498560.0000\n",
      "Training loss (for one batch) at step 2: 28774554927104.0000\n",
      "Training loss (for one batch) at step 3: 29605702402048.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297337171968.0000\n",
      "Training loss (for one batch) at step 2: 28774477332480.0000\n",
      "Training loss (for one batch) at step 3: 29605675139072.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297320394752.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605673041920.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297320394752.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605673041920.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n",
      "0\n",
      "Training loss (for one batch) at step 1: 29297318297600.0000\n",
      "Training loss (for one batch) at step 2: 28774471041024.0000\n",
      "Training loss (for one batch) at step 3: 29605670944768.0000\n"
     ]
    }
   ],
   "source": [
    "content_path = tf.keras.utils.get_file('belfry.jpg','https://storage.googleapis.com/khanhlvg-public.appspot.com/arbitrary-style-transfer/belfry-2611573_1280.jpg')\n",
    "\n",
    "s = train_style_model('../style_images/41176 copy.jpg', '/Users/clayolsen1/ML_folder/Style_transfering_NN/pre_made_style_transfer/content_path/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 14:46:25.047121: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: ../style_images/41176 copy.jpg; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} ../style_images/41176 copy.jpg; No such file or directory [Op:ReadFile]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/clayolsen1/ML_folder/Style_transfering_NN/pre_made_style_transfer/test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/clayolsen1/ML_folder/Style_transfering_NN/pre_made_style_transfer/test.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/clayolsen1/ML_folder/Style_transfering_NN/pre_made_style_transfer/test.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#Load and reshape style image\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/clayolsen1/ML_folder/Style_transfering_NN/pre_made_style_transfer/test.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m style_image \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39;49mload_img(style_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/clayolsen1/ML_folder/Style_transfering_NN/pre_made_style_transfer/test.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#blurr style image a bit to ignore some content capture\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/clayolsen1/ML_folder/Style_transfering_NN/pre_made_style_transfer/test.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m style_image \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mavg_pool(style_image, ksize \u001b[39m=\u001b[39m [\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m], strides \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m], padding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSAME\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/ML_folder/Style_transfering_NN/pre_made_style_transfer/preprocess_util.py:9\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path_to_img)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_img\u001b[39m(path_to_img):\n\u001b[1;32m      8\u001b[0m     max_dim \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m\n\u001b[0;32m----> 9\u001b[0m     img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mread_file(path_to_img)\n\u001b[1;32m     10\u001b[0m     img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mdecode_image(img, channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     11\u001b[0m     img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mconvert_image_dtype(img, tf\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/io_ops.py:133\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mio.read_file\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mio.read_file\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mread_file\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_file\u001b[39m(filename, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     98\u001b[0m   \u001b[39m\"\"\"Reads the contents of file.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[39m  This operation returns a tensor with the entire contents of the input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m    A tensor of dtype \"string\", with the file contents.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_io_ops\u001b[39m.\u001b[39;49mread_file(filename, name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py:581\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    579\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m   \u001b[39mreturn\u001b[39;00m read_file_eager_fallback(\n\u001b[1;32m    582\u001b[0m       filename, name\u001b[39m=\u001b[39;49mname, ctx\u001b[39m=\u001b[39;49m_ctx)\n\u001b[1;32m    583\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n\u001b[1;32m    584\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/gen_io_ops.py:604\u001b[0m, in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    602\u001b[0m _inputs_flat \u001b[39m=\u001b[39m [filename]\n\u001b[1;32m    603\u001b[0m _attrs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m _result \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49mexecute(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mReadFile\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m, inputs\u001b[39m=\u001b[39;49m_inputs_flat,\n\u001b[1;32m    605\u001b[0m                            attrs\u001b[39m=\u001b[39;49m_attrs, ctx\u001b[39m=\u001b[39;49mctx, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n\u001b[1;32m    607\u001b[0m   _execute\u001b[39m.\u001b[39mrecord_gradient(\n\u001b[1;32m    608\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReadFile\u001b[39m\u001b[39m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__ReadFile_device_/job:localhost/replica:0/task:0/device:CPU:0}} ../style_images/41176 copy.jpg; No such file or directory [Op:ReadFile]"
     ]
    }
   ],
   "source": [
    "\n",
    "import NN_package as nn_package\n",
    "import matplotlib.pyplot as plt\n",
    "content_path = tf.keras.utils.get_file('belfry.jpg','https://storage.googleapis.com/khanhlvg-public.appspot.com/arbitrary-style-transfer/belfry-2611573_1280.jpg')\n",
    "\n",
    "style_path = '../style_images/41176 copy.jpg'\n",
    "batch_size = 1\n",
    "#Load and reshape style image\n",
    "style_image = util.load_img(style_path)\n",
    "#blurr style image a bit to ignore some content capture\n",
    "style_image = tf.nn.avg_pool(style_image, ksize = [3,3], strides = [1,1], padding = 'SAME')\n",
    "style_image = util.preprocess_image(style_image, 256)\n",
    "\n",
    "batch_size = 1\n",
    "batch_shape = (batch_size, 256, 256, 3)\n",
    "style_shape = tf.squeeze(style_image).shape\n",
    "\n",
    "\n",
    "content_image = util.load_img(content_path)\n",
    "content_image = util.preprocess_image(content_image, 256)\n",
    "test_image = s.predict(content_image)\n",
    "def imshow(image, title = None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis = 0)\n",
    "        plt.imshow(image)\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "test_image= test_image/255\n",
    "#imshow(content_image)\n",
    "#test_image = tf.clip_by_value(test_image, clip_value_min=0.0, clip_value_max=1.0)\n",
    "imshow(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import NN_package as nn\n",
    "\n",
    "\n",
    "style_shape = tf.squeeze(style_image).shape\n",
    "X_content = tf.keras.Input(name = 'X_Content', shape = style_shape, batch_size=batch_size)\n",
    "stlized = nn.stylelizer(X_content)\n",
    "tf.keras.Model(X_content, stlized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
